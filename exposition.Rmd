---
title: "Gaussian process models"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: In this report, I show some basic theory and applications for gaussian processes.

output:
  html_document:
    toc: yes
---

```{r message=F, echo=F}
rm(list=ls())
library("knitr")
read_chunk("./gp.R")
```

```{r load_libraries, echo=F}
```

# Introduction
## Deffinition of a gaussian process
A gaussian process $GP$ over a set $S$ is a continuum of random variables
$X_{\tau^\star} = \{X_\tau\}$ for $\tau\in\tau^\star\subset\mathbf{R}$ where any
finite subset
$X_T$ where $T = \{t_1,\dots,t_n\}$ where $t_i\in\tau^\star$ for all
$i\in\{1,\dots,n\}$ are jointly distributed according to a multivariate gaussian
distribution. You can think about this like a distributions over functions over
$S$. First, I will consider $S=\mathbf{R}$, forcing the gaussian process to
induce a distribution over functions over the reals. A handful of draws from
this induced distribution are depicted below.

## Baby's first visualization
Below I depict discretized draws from a gaussian process.

```{r set_params1}
```

```{r plot_basic_gp, echo=F}
```

  
You may notice, browsing this code or perceptively reading the introduction,
that I've left a bit out of the specification of the particular gaussian process
depicted above.

## A bit more on the deffiniton
A key to the behavior of the process is its covariance kernel.
Here, I used a cannonical exponential squared loss
$k(x_1, x_2) = e^{-\sigma((x_1-x_2)/\ell)^2}$ with a couple tuning
hyperparameters $\sigma$ and $\ell$. Computing the outer product of the $x$
values of interest
(here just $D=`r D`$ points equally spaced between $`r t[1]`$ and $`r t[2]`$)
using the kernel yeilds a $D\times D$ covariance matrix.

An important point here is that each discretized point on the $X$ axis is, in
the eyes of the gaussian process, an orthogonal dimension. Thus induced by the
gaussian process is a multivariate normal distribution over $D$ variables. The
covariance of these dimensions is induced by the kernel, and in this case, under
the exponential squared loss, that means that points close in the natural space
are highly correlated dimensions in the induced distribution. It's this property
that gives us the probability of a particular function being drawn from the
induced distribution proportional to its smoothness.

## An alternative kernel
Before I get too stuck on the exponential square loss, however useful and
traditional it may be, it's important to look at different kernels. These
different kernels will yeild different covariance matricies, but all kerenels
share an important property: they induce positive semidefinite covariance
matricies. If this semidefinite stuff is meaningless to you, look it up, but
also it basically means that the kernel induces a non-negative way to measure
distances between points in the natural space. It's not a metric, but it's still
useful to think about the particular square exponential loss kernel like a
measuring stick - it gives high values for close things, and low values for far
things.

Now, this isn't neccessarily true for a kernel - remember that it just
has to induce positive semidefinite covaraince matricies. To drive this home
let's look at a different kernel:

```{r alt_kernel}
```

```{r plot_01_kernel, echo=F}
```

  
Ugly, right? This is because the 0-1 loss kernel forces the covariance matrix to
be the identity matrix, inducing a spherical gaussian (each dimension is
independent) in $D$ dimensions. So let's turn this ugly plot on its side to see
that this is indeed the case. (By turning it on its side, I mean that we plot a
density of the Y values irrespective of the X values to see that they are more
or less gaussian.)

```{r plot_01_kernel_sideways, echo=F}
```

## Another alternative kernel

For good measure, let's look at another common kernel

```{r abs_kernel}
```

```{r plot_abs_kernel, echo=F}
```

# And now a simple application to regression
You may have noticed that I'm using a constant mean function
$\mu(x) = 0$ for all $x$. Now, I'll switch to a regression context where I'll
look at drawing sample functions from a gaussian process with a mean function
set to the observed values of the function.

## Target function deffinition
For the sake of example, I'll consider the target function
$$
y = f(x) + \epsilon \quad {\rm where} \quad
f(x) = x^2 e^{-x^2} + {\rm sin}(x) \quad {\rm and} \quad
\epsilon \sim {\rm iid} N(0,\sigma_\epsilon)
$$

```{r gaus_spline}
```

```{r plot_regression_setup, echo=F}
```

  
Next, I'll try to regress a point not in the sample using gaussian proceess
regression a.k.a. kreiging. The unknown point will be at $x=`r xu`$ depicted
above by the violet vertical line.

## Regression at a point outside the support
To regress this point, I'm interested in the posterior distribution
$f(y_\star \mid \mathbf{y})$.
By the definition of the gaussian process, we know that the joint distribution
of the unknown and known points is
$f(y_\star, \mathbf{y}) = N(0, \mathbf{K})$ where $\mathbf{K}$ is calculated
using the kernel. Ommitting the derrivation for now, we use the fact that the
posterior takes the following form:

```{r kreiging}
```

```{r plot_posterior, echo=F}
```

  
The 64% credible interval for the estimated value at
$x=`r xu`$ is $`r mean(post_obs)`\pm`r sqrt(var(post_obs))`$.
The true value using the model introduced at the beginning of this section is
$`r mu(xu)`$. Notice that this is wide, but it does contain the true value.

## Posterior 90% credible band
Here I calculate the 90% posterior credible band which serves as a reigon of
reasonable confidence for the location of the target curve. I calculate this
band by drawing from the posterior at many points along the domain of the
target function inside and beyond its sampling support. Insodoing I gather a
sample of potential curves and calculate the emperical quantiles to recover the
confidence band. Note that the analytical posterior 90% confidence band is
trivially accessible via the gaussian process assumption and the form of the
posterior distriution - the results shouldn't be all that different in one
dimension where the law of large numbers works quickly.

```{r credible_band}
```

```{r plot_credible_band, echo=F}
```

## Tuning the model
The astute reader already noticed my arbitrary selection of the hyperparameters
$\sigma_k = `r sigkern`$ and $\ell = `r lkern`$ in the squared exponential loss
kernel above, which is, for refference:
$$
k(x_1, x_2) = e^{-\frac{\sigma_k^2}{2}\left(\frac{x_1-x_2}{\ell}\right)^2}
$$

Let $\theta = (\sigma_k, \ell)$ Using the fact that, by our gaussian process
definition,
$$
p(y|x, \theta) = N(0, \Sigma)
$$
where $\Sigma$ is generated by taking the outer product of $x$ with itself using
the kernel. Because I have only two parameters to tune and I know vaguely
where to start looking, I'll use a quick and dirty grid search to maximize the
log likelihood
$$
\log p(y|x, \theta) =
\frac{-1}{2} y^\top K^{-1} y - \frac{1}{2}\log(|K|) - \frac{n}{2}\log(2\pi)
$$

Maximizing the log likelihood is equivalent to minimizing the log negative log
likelihood, and this is the optimization I perform below.

```{r tune_model}
```

```{r plot_parameters, echo=F}
```

  
With the optimizing parameters $\sigma_k = `r sigopt`$ in purple and
$\ell = `r ellopt`$ in red, I perform the gaussian process regression again to
see what better parameters do for the accuracy of the problem.

```{r draw_tuned_post}
```

```{r plot_tuned_post, echo=F}
```

As you can see, tuning the model helps keep the band reasonably tight, although
the mean function doesn't seem to shift all that much.

A final note before moving on: there's a good way to account for the noise
$\epsilon$ in the model, outlined in source [3], if you have a priori a good
sense of the level of the noise $\sigma_\epsilon$. It is possible to estimate
this noise using something like leave-one-out cross validation but I've ommitted
this proceedure.

# 2D gaussian process
A 2D GP is not much different from a 1D GP. The domain is similarly discretized
and the covariance matrix is computed as $s_{ij} = f(\|x_i-x_j\|)$ for
$x_{i,j} \textrm{ each }\in \mathbb{R}^2$.

```{r gp2d}
```

# Sources
1. https://en.wikipedia.org/wiki/Gaussian_process
2. https://arxiv.org/abs/1505.02965
3. http://www.gaussianprocess.org/gpml/chapters/RW2.pdf
