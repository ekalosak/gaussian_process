---
title: "Gaussian Process"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: I'll run you through the basics and some extensions on both the theory of Gaussian Processes and their implementation in a simple, readable R script

output:
  html_document:
    toc: yes
---

```{r message=F, echo=F}
rm(list=ls())
library("knitr")
read_chunk("./gp.R")
```

```{r load_libraries, echo=F}
```

# Introduction
## Deffinition of a Gaussian Process
A Gaussian Process is a set of random variables $S=\{X_\tau | \tau \in T\}$
indexed by a set $T$, where usually $T \subseteq \mathbb{R}$ where any
finite subset $s \subset S, card(s) < \infty$ of random variables are jointly
normally distributed. That is, if you pick e.g. two $X_\tau$ from $S$, they'll
be distrubted under the joint distribution e.g.
$$
f_{X_1, X_2}(x_1, x_2) =
\frac{1}{2\pi\sigma^2} e^{-1/(2\sigma^2)*(x_1^2 + x_2^2)}
$$
A handful of draws from a simple GP are depicted below.

## Baby's first visualization

```{r set_params1}
```

```{r plot_basic_gp, echo=F}
```

  
You may notice, browsing this code or perceptively reading the introduction,
that I've left a bit out of the specification of the particular Gaussian Process
depicted above.

## A bit more on the deffiniton
A key to the behavior of the process is its covariance kernel.
Above, I used a cannonical exponential squared loss
$k(x_1, x_2) = e^{-\sigma((x_1-x_2)/\ell)^2}$ with a couple tuning
hyperparameters: $\sigma$ and $\ell$. Computing the outer product of the $x$
values of interest
(here just $D=`r D`$ points equally spaced between $`r t[1]`$ and $`r t[2]`$)
using the kernel yeilds a $D\times D$ covariance matrix:

$$
\Sigma_s = \[k(x_i, x_j)\]_{i,j \in T}
$$

An important point here is that each discretized point on the $X$ axis is, in
the eyes of the Gaussian Process, an orthogonal dimension. Thus induced by the
Gaussian Process is a multivariate normal distribution over $D$ variables. The
covariance of these dimensions is induced by the kernel, and in this case, under
the exponential squared loss, that means that points close in the natural space
are highly correlated dimensions in the induced distribution. It's this property
that gives us the probability of a particular function being drawn from the
induced distribution proportional to its smoothness.

## An alternative kernel
Before I get too stuck on the exponential square loss, however useful and
traditional it may be, it's important to look at different kernels. These
different kernels will yeild different covariance matricies, but all kerenels
share an important property: they induce positive semidefinite covariance
matricies. If this semidefinite stuff is meaningless to you, look it up, but
also it basically means that the kernel induces a non-negative way to measure
distances between points in the natural space. It's not a metric, but it's still
useful to think about the particular square exponential loss kernel like a
measuring stick - it gives high values for close things, and low values for far
things.

Now, this isn't neccessarily true for a kernel - remember that it just
has to induce positive semidefinite covaraince matricies. To drive this home
let's look at a different kernel:

```{r alt_kernel}
```

```{r plot_01_kernel, echo=F}
```

  
Ugly, right? This is because the 0-1 loss kernel forces the covariance matrix to
be the identity matrix, inducing a spherical Gaussian (each dimension is
independent) in $D$ dimensions. So let's turn this ugly plot on its side to see
that this is indeed the case. (By turning it on its side, I mean that we plot a
density of the Y values irrespective of the X values to see that they are more
or less Gaussian.)

```{r plot_01_kernel_sideways, echo=F}
```

## Another alternative kernel

For good measure, let's look at another common kernel

```{r abs_kernel}
```

```{r plot_abs_kernel, echo=F}
```

# And now a simple application to regression
You may have noticed that I'm using a constant mean function
$\mu(x) = 0$ for all $x$. Now, I'll switch to a regression context where I'll
look at drawing sample functions from a Gaussian Process with a mean function
set to the observed values of the function.

## Target function deffinition
For the sake of example, I'll consider the target function
$$
y = f(x) + \epsilon \quad {\rm where} \quad
f(x) = x^2 e^{-x^2} + {\rm sin}(x) \quad {\rm and} \quad
\epsilon \sim {\rm iid} N(0,\sigma_\epsilon)
$$

```{r gaus_spline}
```

```{r plot_regression_setup, echo=F}
```

  
Next, I'll try to regress a point not in the sample using Gaussian proceess
regression a.k.a. kreiging. The unknown point will be at $x=`r xu`$ depicted
above by the violet vertical line.

## Regression at a point outside the support
To regress this point, I'm interested in the posterior distribution
$f(y_\star \mid \mathbf{y})$.
By the definition of the Gaussian Process, we know that the joint distribution
of the unknown and known points is
$f(y_\star, \mathbf{y}) = N(0, \mathbf{K})$ where $\mathbf{K}$ is calculated
using the kernel. Ommitting the derrivation for now, we use the fact that the
posterior takes the following form:

```{r kreiging}
```

```{r plot_posterior, echo=F}
```

  
The 64% credible interval for the estimated value at
$x=`r xu`$ is $`r mean(post_obs)`\pm`r sqrt(var(post_obs))`$.
The true value using the model introduced at the beginning of this section is
$`r mu(xu)`$. Notice that this is wide, but it does contain the true value.

## Posterior 90% credible band
Here I calculate the 90% posterior credible band which serves as a reigon of
reasonable confidence for the location of the target curve. I calculate this
band by drawing from the posterior at many points along the domain of the
target function inside and beyond its sampling support. Insodoing I gather a
sample of potential curves and calculate the emperical quantiles to recover the
confidence band. Note that the analytical posterior 90% confidence band is
trivially accessible via the Gaussian Process assumption and the form of the
posterior distriution - the results shouldn't be all that different in one
dimension where the law of large numbers works quickly.

```{r credible_band}
```

```{r plot_credible_band, echo=F}
```

## Tuning the model
The astute reader already noticed my arbitrary selection of the hyperparameters
$\sigma_k = `r sigkern`$ and $\ell = `r lkern`$ in the squared exponential loss
kernel above, which is, for refference:
$$
k(x_1, x_2) = e^{-\frac{\sigma_k^2}{2}\left(\frac{x_1-x_2}{\ell}\right)^2}
$$

Let $\theta = (\sigma_k, \ell)$ Using the fact that, by our Gaussian Process
definition,
$$
p(y|x, \theta) = N(0, \Sigma)
$$
where $\Sigma$ is generated by taking the outer product of $x$ with itself using
the kernel. Because I have only two parameters to tune and I know vaguely
where to start looking, I'll use a quick and dirty grid search to maximize the
log likelihood
$$
\log p(y|x, \theta) =
\frac{-1}{2} y^\top K^{-1} y - \frac{1}{2}\log(|K|) - \frac{n}{2}\log(2\pi)
$$

Maximizing the log likelihood is equivalent to minimizing the log negative log
likelihood, and this is the optimization I perform below.

```{r tune_model}
```

```{r plot_parameters, echo=F}
```

  
With the optimizing parameters $\sigma_k = `r sigopt`$ in purple and
$\ell = `r ellopt`$ in red, I perform the Gaussian Process regression again to
see what better parameters do for the accuracy of the problem.

```{r draw_tuned_post}
```

```{r plot_tuned_post, echo=F}
```

As you can see, tuning the model helps keep the band reasonably tight, although
the mean function doesn't seem to shift all that much.

A final note before moving on: there's a good way to account for the noise
$\epsilon$ in the model, outlined in source [3], if you have a priori a good
sense of the level of the noise $\sigma_\epsilon$. It is possible to estimate
this noise using something like leave-one-out cross validation but I've ommitted
this proceedure.

# 2D Gaussian Process
A 2D GP is not much different from a 1D GP. The domain is similarly discretized
and the covariance matrix is computed as $s_{ij} = f(\|x_i-x_j\|)$ for
$x_{i,j} \textrm{ each }\in \mathbb{R}^2$.

```{r gp2d}
```

```{r plot_gp2d, echo=F}
```

The above plots are draws from a 2D Gaussian Process with a mean function
$\mu(x_1,x_2)=0$ for simplicity.

# Sources
1. Wikipedia https://en.wikipedia.org/wiki/Gaussian_process
2. Mark Ebden; "Gaussian Processes: A Quick Introduction"
   https://arxiv.org/abs/1505.02965
3. C. E. Rasmussen & C. K. I. Williams; "Gaussian Processes for Machine
   Learning; Chapter 2 on Regression", the MIT Press, 2006, ISBN 026218253X
   http://www.gaussianprocess.org/gpml/chapters/RW2.pdf
