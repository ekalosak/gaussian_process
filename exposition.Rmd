---
title: "Gaussian process models"
author:
- Eric Kalosa-Kenyon
date: "`r format(Sys.time(), '%d %B %Y')`"

abstract: In this report, I show some basic theory and applications for gaussian processes.

output:
  html_document:
    toc: yes
---

```{r echo=F}
rm(list=ls())
set.seed(33)

library(ggplot2)
library(MASS)
library(reshape)
library(latex2exp)
```

# Introduction
## Deffinition of a gaussian process
A gaussian process $GP$ over a set $S$ is a continuum of random variables
$X_{\tau^\star} = \{X_\tau\}$ for $\tau\in\tau^\star\subset\mathbf{R}$ where any finite subset
$X_T$ where $T = \{t_1,\dots,t_n\}$ where $t_i\in\tau^\star$ for all
$i\in\{1,\dots,n\}$ are jointly distributed according to a multivariate gaussian
distribution. You can think about this like a distributions over functions over
$S$. First, I will consider $S=\mathbf{R}$, forcing the gaussian process to
induce a distribution over functions over the reals. A handful of draws from
this induced distribution are depicted below.

## Baby's first visualization
Below I depict discretized draws from a gaussian process.

```{r}
# Parameters
t = c(-pi,pi) # plotting range
D = 20*floor(abs(t[1]-t[2])) # number of evenlt spaced collocation points
K = 10 # number of curves
lkern = 0.2 # kernel length-scale hyperparameter
sigkern = 2.7 # kernel varaince hyperparameter

# Define kernel
kernel_sel = function(x0, x1){
    # Using squared exponential loss
    return(sigkern^2 * exp(-1/2 * (x0-x1)^2 / lkern))
}

# Define mean function
mu = function(x){
    # Constant mean
    return(0*x)
}

# Collocate X values
xs = seq(t[1], t[2], length.out=D)

# Calculate covariance matrix
S = outer(xs, xs, kernel_sel)

# Draw joint multivariate gaussian samples
ys = t(mvrnorm(K, mu = mu(xs), Sigma=S))

# Plot
plt_df = data.frame(x=xs, y=ys)
names(plt_df) = c("x", paste("GP", 1:K, sep=""))
plt_df_m = melt(plt_df, id=c("x"))
plt = ggplot(plt_df_m) +
    geom_line(aes(x=x, y=value, color=variable)) +
    labs(x="X", y="Y",
         title=paste(K, "draws from a gaussian process"),
         color="Draw number")
```

```{r echo=F}
plt
```
  
You may notice, browsing this code or perceptively reading the introduction,
that I've left a bit out of the specification of the particular gaussian process
depicted above.

## A bit more on the deffiniton
A key to the behavior of the process is its covariance kernel.
Here, I used a cannonical exponential squared loss
$k(x_1, x_2) = e^{-\sigma((x_1-x_2)/\ell)^2}$ with a couple tuning
hyperparameters $\sigma$ and $\ell$. Computing the outer product of the $x$
values of interest
(here just $D=`r D`$ points equally spaced between $`r t[1]`$ and $`r t[2]`$)
using the kernel yeilds a $D\times D$ covariance matrix.

An important point here is that each discretized point on the $X$ axis is, in
the eyes of the gaussian process, an orthogonal dimension. Thus induced by the
gaussian process is a multivariate normal distribution over $D$ variables. The
covariance of these dimensions is induced by the kernel, and in this case, under
the exponential squared loss, that means that points close in the natural space
are highly correlated dimensions in the induced distribution. It's this property
that gives us the probability of a particular function being drawn from the
induced distribution proportional to its smoothness.

## An alternative kernel
Before I get too stuck on the exponential square loss, however useful and
traditional it may be, it's important to look at different kernels. These
different kernels will yeild different covariance matricies, but all kerenels
share an important property: they induce positive semidefinite covariance
matricies. If this semidefinite stuff is meaningless to you, look it up, but
also it basically means that the kernel induces a non-negative way to measure
distances between points in the natural space. It's not a metric, but it's still
useful to think about the particular square exponential loss kernel like a
measuring stick - it gives high values for close things, and low values for far
things.

Now, this isn't neccessarily true for a kernel - remember that it just
has to induce positive semidefinite covaraince matricies. To drive this home
let's look at a different kernel:

```{r}
kernel_01 = function(x0, x1){
    # Using 0-1 loss
    return((x0 == x1) + 0)
}

# Calculate covariance matrix
S = outer(xs, xs, kernel_01)

# Draw joint multivariate gaussian samples
ys = t(mvrnorm(K, mu = mu(xs), Sigma=S))

# Plot
plt_df = data.frame(x=xs, y=ys)
names(plt_df) = c("x", paste("GP", 1:K, sep=""))
plt_df_m = melt(plt_df, id=c("x"))
plt = ggplot(plt_df_m) +
    geom_line(aes(x=x, y=value, color=variable)) +
    labs(x="X", y="Y",
         title=paste(K, "draws from a gaussian process"),
         color="Draw number")
```

```{r echo=F}
plt
```
  
Ugly, right? This is because the 0-1 loss kernel forces the covariance matrix to
be the identity matrix, inducing a spherical gaussian (each dimension is
independent) in $D$ dimensions. So let's turn this ugly plot on its side to see
that this is indeed the case.

```{r}
plt = ggplot(plt_df_m) +
    geom_freqpoly(aes(value, ..density.., color=variable), bins=floor(D/5)) +
    stat_function(fun = dnorm, colour = "steelblue", size=1.1)
plt
```

## Another alternative kernel

For good measure, let's look at another common kernel

```{r}
kernel_abs = function(x0, x1){
    # Using exponential loss
    return(4^2 * exp(-1/2 * abs(x0-x1) / 0.5))
}

# Calculate covariance matrix
S = outer(xs, xs, kernel_abs)

# Draw joint multivariate gaussian samples
ys = t(mvrnorm(K, mu = mu(xs), Sigma=S))

# Plot
plt_df = data.frame(x=xs, y=ys)
names(plt_df) = c("x", paste("GP", 1:K, sep=""))
plt_df_m = melt(plt_df, id=c("x"))
plt = ggplot(plt_df_m) +
    geom_line(aes(x=x, y=value, color=variable)) +
    labs(x="X", y="Y",
         title=paste(K, "draws from a gaussian process"),
         color="Draw number")
```

```{r echo=F}
plt
```

# And now a simple application to regression
You may have noticed that I'm using a constant mean function
$\mu(x) = 0$ for all $x$. Now, I'll switch to a regression context where I'll
look at drawing sample functions from a gaussian process with a mean function
set to the observed values of the function.

## Target function deffinition
For the sake of example, I'll consider the target function
$$
y = f(x) + \epsilon \quad {\rm where} \quad
f(x) = x^2 e^{-x^2} + {\rm sin}(x) \quad {\rm and} \quad
\epsilon \sim {\rm iid} N(0,\sigma_\epsilon)
$$

```{r}
# Parameters
M = 7 # number of points to observe from the model
# NOTE: that if this gets too big, inverting the covariance matrix can bevome
# numerically unstable resulting in failures to invert (which is ok) or wildly
# divergent estimates (which could be catastrophic).

# Heterogeneous mean function
mu = function(x){
    return(x^2 * exp(-x^2) + sin(x))
}

ys = mu(xs)
# ixs = base::sample(1:D, M)
ixs = ceiling(seq(1, D, length.out=M))
xsobv = xs[ixs]
ysobv = ys[ixs] + rnorm(M, sd=0.4)
xu = t[2] + 1/2

# Plot the true function and the observed points
plt = ggplot(data.frame(x=xs, y=ys)) +
    geom_line(aes(x=x, y=y), color="steelblue", size=1.1) +
    geom_point(data=data.frame(x=xsobv, y=ysobv), aes(x=x, y=y), color="coral") +
    geom_vline(xintercept=xu, color="darkviolet")
```

```{r echo=F}
plt
```
  
Next, I'll try to regress a point not in the sample using gaussian proceess
regression a.k.a. kreiging. The unknown point will be at $x=`r xu`$ depicted
above by the violet vertical line.

## Regression at a point outside the support
To regress this point, I'm interested in the posterior distribution
$f(y_\star \mid \mathbf{y})$.
By the definition of the gaussian process, we know that the joint distribution
of the unknown and known points is
$f(y_\star, \mathbf{y}) = N(0, \mathbf{K})$ where $\mathbf{K}$ is calculated
using the kernel. Ommitting the derrivation for now, we use the fact that the
posterior takes the following form:

```{r}
gen_posterior = function(xu, xs, ys, ker=kernel_sel){
    ## Generates a posterior function

    # posterior probability f(yu | ys) = N(m, s)
    K0 = outer(xs, xs, ker) # kernel of the observed values
    K1 = outer(xu, xs, ker)
    K2 = outer(xu, xu, ker)
    K = rbind(cbind(K0, t(K1)), cbind(K1, K2))

    # mean and variance of posterior
    m = K1 %*% solve(K0) %*% ys
    s = K2 - K1 %*% solve(K0) %*% t(K1)

    return(
           function(n){
               mvrnorm(n, mu = m, Sigma = s)
           }
    )
}

posterior = gen_posterior(xu, xsobv, ysobv)

n = 100
post_obs = posterior(n)
plt_post = ggplot(data=data.frame(y=post_obs)) +
    geom_histogram(aes(x=y), bins=floor(n/4))
plt_post
```
  
The 64% credible interval for the estimated value at
$x=`r xu`$ is $`r mean(post_obs)`\pm`r sqrt(var(post_obs))`$.
The true value using the model introduced at the beginning of this section is
$`r mu(xu)`$. Notice that this is wide, but it does contain the true value.

## Posterior 90% credible band
Here I calculate the 90% posterior credible band which serves as a reigon of
reasonable confidence for the location of the target curve. I calculate this
band by drawing from the posterior at many points along the domain of the
target function inside and beyond its sampling support. Insodoing I gather a
sample of potential curves and calculate the emperical quantiles to recover the
confidence band. Note that the analytical posterior 90% confidence band is
trivially accessible via the gaussian process assumption and the form of the
posterior distriution - the results shouldn't be all that different in one
dimension where the law of large numbers works quickly.

```{r}
# Parameters
m = 50 # resolution of the posterior
n = 10 # number of draws from the posterior
d = t[2] - t[1] # length of the sampling support

# Collocated posterior points
xus = seq(t[1] - d/10, t[2] + d/10, length.out=m)

# Draw from the posterior for an illustrative plot
posterior_multi = gen_posterior(xus, xsobv, ysobv)
draws = posterior_multi(n)

# Plot the aforementioned illustrative plot of draws from the posterior
df_post = data.frame(t(rbind(draws, xus)))
names(df_post) = c(paste("Draw", 1:n), "X")
df_post_m = melt(df_post, id="X")
plt_post = ggplot(df_post_m) +
    geom_line(aes(x=X, y=value, color=variable)) +
    geom_point(data=data.frame(x=xsobv, y=ysobv),
               aes(x=x, y=y),
               color="coral") +
    labs(x="X", y="Y", title=paste(n, "draws from the posterior"),
         color="Draw number")

# Draw for the emperical 90% band and calculate the band
n = 50
draws = posterior_multi(n)
qs = apply(t(draws), 1, (function(x) (return(quantile(x, c(0.05, 0.95))))))

# Plot the 90% band
df_post_band = data.frame(t(rbind(qs, xus)))
names(df_post_band) = c("q0", "q1", "X")
plt_post_band = ggplot(df_post_band) +
    geom_ribbon(aes(ymin=q0, ymax=q1, x=X), alpha=0.6) +
    geom_point(data=data.frame(x=xsobv, y=ysobv),
               aes(x=x, y=y),
               color="coral") +
    geom_line(data=data.frame(x=xs, y=ys), aes(x=x, y=y), color="steelblue") +
    labs(x="X", y="Y", title=paste("90% credible band using", n, "draws"))
```

```{r echo=F}
plt_post
plt_post_band
```

## Tuning the model
The astute reader already noticed my arbitrary selection of the hyperparameters
$\sigma_k = `r sigkern`$ and $\ell = `r lkern`$ in the squared exponential loss
kernel above, which is, for refference:
$$
k(x_1, x_2) = e^{-\frac{\sigma_k^2}{2}\left(\frac{x_1-x_2}{\ell}\right)^2}
$$

Let $\theta = (\sigma_k, \ell)$ Using the fact that, by our gaussian process
definition,
$$
p(y|x, \theta) = N(0, \Sigma)
$$
where $\Sigma$ is generated by taking the outer product of $x$ with itself using
the kernel. Because I have only two parameters to tune and I know vaguely
where to start looking, I'll use a quick and dirty grid search to maximize the
log likelihood
$$
\log p(y|x, \theta) =
\frac{-1}{2} y^\top K^{-1} y - \frac{1}{2}\log(|K|) - \frac{n}{2}\log(2\pi)
$$

Maximizing the log likelihood is equivalent to minimizing the log negative log
likelihood, and this is the optimization I perform below.

```{r}
## Grid search to maximize the log likelihood
kern = function(x0, x1, sig, ell){
    # Using squared exponential loss
    return(sig^2 * exp(-1/2 * (x0-x1)^2 / ell))
}

loglik = function(sig, ell){
    kern_sl = function(x0, x1){
        return(kern(x0, x1, sig, ell))
    }
    x = xsobv
    y = ysobv
    n = length(x)
    K = outer(x, x, kern_sl)
    r = -1/2*t(y)%*%solve(K)%*%y - 1/2*log(det(K)) - n/2*log(pi*2)
    return(r)
}

## Parameterize and calculate log likelihoods
G = 50 # resolution of gridsearch
sigbs = c(0.5, 1.5) # bounds on parameters
ellbs = c(0.01, 1.5) # bounds on parameters
sigs = log(seq(exp(sigbs[1]), exp(sigbs[2]), length.out=G))
ells = log(seq(exp(ellbs[1]), exp(ellbs[2]), length.out=G))
params = expand.grid(sigs, ells) # generate all pairs of sigs and ells
names(params) = c("sig", "ell")

params$loglik = apply(params, 1, (function(r)(return(loglik(r[1], r[2])))))
params$loglik = log(-params$loglik)

# Plot the grid search for parameters
ix = which(params$loglik == min(params$loglik))[1]
sigopt = params$sig[ix]
ellopt = params$ell[ix]
plt_params = ggplot(data=params, aes(x=sig, y=ell, z=loglik, fill=loglik)) +
    geom_contour(aes(color=loglik), binwidth=0.05, color="steelblue") +
    geom_hline(aes(yintercept=ellopt), color="tomato") +
    geom_vline(aes(xintercept=sigopt), color="darkviolet") +
    labs(x=TeX("$\\sigma_k$"),
         y="l",
         title="Grid search for optimal kernel parameters")
```

```{r echo=F}
plt_params
```
  
With the optimizing parameters $\sigma_k = `r sigopt`$ in purple and
$\ell = `r ellopt`$ in red, I perform the gaussian process regression again to
see what better parameters do for the accuracy of the problem.

```{r echo=F}
# Parameters
m = 50 # resolution of the posterior
n = 10 # number of draws from the posterior
d = t[2] - t[1] # length of the sampling support

# Collocated posterior points
xus = seq(t[1] - d/10, t[2] + d/10, length.out=m)

# Draw for the emperical 90% band and calculate the band
kern_sel_opt = function(x0, x1){
    return(sigopt^2 * exp(-1/2 * (x0-x1)^2 / ellopt))
}
posterior_opt = gen_posterior(xus, xsobv, ysobv, ker=kern_sel_opt)
draws = posterior_opt(n)
qs = apply(t(draws), 1, (function(x) (return(quantile(x, c(0.05, 0.95))))))

# Plot the aforementioned illustrative plot of draws from the posterior
df_post = data.frame(t(rbind(draws, xus)))
names(df_post) = c(paste("Draw", 1:n), "X")
df_post_m = melt(df_post, id="X")
plt_post = ggplot(df_post_m) +
    geom_line(aes(x=X, y=value, color=variable)) +
    geom_point(data=data.frame(x=xsobv, y=ysobv),
               aes(x=x, y=y),
               color="coral") +
    labs(x="X", y="Y", title=paste(n, "draws from the optimized posterior"),
         color="Draw number")


# Plot the 90% band
n = 50 # number of draws from the posterior
draws = posterior_opt(n)
qs = apply(t(draws), 1, (function(x) (return(quantile(x, c(0.05, 0.95))))))
df_post_band = data.frame(t(rbind(qs, xus)))
names(df_post_band) = c("q0", "q1", "X")
plt_post_band = ggplot(df_post_band) +
    geom_ribbon(aes(ymin=q0, ymax=q1, x=X), alpha=0.6) +
    geom_point(data=data.frame(x=xsobv, y=ysobv),
               aes(x=x, y=y),
               color="coral") +
    geom_line(data=data.frame(x=xs, y=ys), aes(x=x, y=y), color="steelblue") +
    labs(x="X", y="Y", title=paste("90% credible band using", n, "draws"))

plt_post
plt_post_band
```

As you can see, tuning the model helps keep the band reasonably tight, although
the mean function doesn't seem to shift all that much.

A final note before moving on: there's a good way to account for the noise
$\epsilon$ in the model, outlined in source [3], if you have a priori a good
sense of the level of the noise $\sigma_\epsilon$. It is possible to estimate
this noise using something like leave-one-out cross validation but I've ommitted
this proceedure.

# 2D gaussian process
Coming soon

# Sources
1. https://en.wikipedia.org/wiki/Gaussian_process
2. https://arxiv.org/abs/1505.02965
3. http://www.gaussianprocess.org/gpml/chapters/RW2.pdf
